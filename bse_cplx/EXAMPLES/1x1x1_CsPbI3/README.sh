# Daniel Chabeda 12.14.25
#

# This directory contains the necessary files to run a
# solver for the Bethe-Salpether equation in the Tamm-Dancoff
# approximation with static screening. It will take a basis
# of electron and hole quasiparticle states (generated by
# Filter Diagonalization) and compute correlated excitons
# as a linear combination of electron-hole product states.

# Thus, the major convergence parameter is the size of the
# electron/hole basis. The number of electrons and holes is
# set in ./bse/input.par, and is currently 8e/8h. Typical
# calculations for lead halide perovskites require ~100 electrons
# and holes to converge exciton energies and optical properties.

# To run the code, navigate to the ./bse directory and
# 1) link the output.dat file from the filter job
# ln -s PATH_TO_FILTER/filter/output.dat .
# 2) Link the bse_cplx.x executable
# ln -s PATH_TO_EXEC/bse_cplx.x .

# and then run the code with the command
# srun -n 1 ./bse_cplx.x | tee -a run.dat

# The entire code uses an MPI/OpenMP parallelization framework.
# Additional OpenMP threads accelerate calculation of optical
# matrix elements, Fast Fourier Transforms,and two-electron integrals.
# The bottleneck of the code is the calculation of two-electron
# Coulomb integrals (N^4 scaling). MPI implementation splits the
# calculation of integrals onto different nodes. Using more nodes
# is the best way to scale a calculation with many basis states.
# For calculations with few basis states (less than 64 e- or h+ states)
# a single node can still be efficient. For large NCs with huge grids,
# using many threads to parallelize the FFT accelerates the computation.

An example for parallelizing a large job (160e/160h) would be to use
8 nodes, each with 2 MPI ranks for a total of 16 MPI ranks. On a 256 CPU
machine, this leaves 128 CPUs-per-rank -> 64 OpenMP threads on a dual-core machine.
You would set nThreads = 64 in bse/input.par and run
srun -n 16 -c 128 --cpu-bind=cores ./bse_cplx.x
or
mpirun -n 16 -c 128 --cpu-bind=cores ./bse_cplx.x
